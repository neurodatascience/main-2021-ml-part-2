* export options                                                   :noexport:
** general
   #+STARTUP: beamer
   #+OPTIONS: H:3 toc:nil num:t date:nil

   #+LaTeX_CLASS: beamer
   #+LaTeX_CLASS_OPTIONS: [presentation,mathserif,table]

** presentation info
   #+TITLE: Machine learning Part 2
   # #+AUTHOR: Jérôme Dockès

   #+BEAMER_HEADER: \author{Jérôme Dockès \& Nikhil Bhagwat}
   # #+BEAMER_HEADER: \titlegraphic{\includegraphics[height=1.5cm]{figures/mcgill-university.png} \hspace{1.5cm} \includegraphics[height=1.5cm]{figures/origami-lab-logo.png}}
   #+BEAMER_HEADER: \date{MAIN tutorials course 2021-11-26}
   #+BEAMER_HEADER: \subtitle{Model selection \& validation}

** latex headers
*** fonts and beamer
    #+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty

    #+LaTeX_HEADER: \usepackage[T1]{fontenc}

    #+LaTeX_HEADER: \usepackage{DejaVuSans}
    # #+LaTeX_HEADER: \usepackage{DejaVuSansMono}

    # #+LaTeX_HEADER: \usepackage[default]{opensans}
    # #+LaTeX_HEADER: \usepackage{lmodern}
    # #+LaTeX_HEADER: \usepackage{libertine}
    # #+LaTeX_HEADER: \usepackage{iwona}
    # #+LaTeX_HEADER: \usepackage[sc,osf]{mathpazo}
    # #+LaTeX_HEADER: \usepackage{mathptmx}
    # #+LaTeX_HEADER: \usepackage{helvet}
    # #+LaTeX_HEADER: \usefonttheme{default}

    # #+LaTeX_HEADER: \usefonttheme{serif}
    #+LaTeX_HEADER: \usefonttheme{professionalfonts}

    #+LaTeX_HEADER: \usepackage[euler-digits,euler-hat-accent]{eulervm}

    # #+LaTeX_HEADER: \setbeamertemplate{itemize items}[circle]
    #+LaTeX_HEADER: \setbeamertemplate{itemize items}{•}
    #+LaTeX_HEADER: \setbeamertemplate{enumerate items}[default]

    #+LaTex_HEADER: \AtBeginSection[]
    #+LaTex_HEADER: {
    #+LaTex_HEADER: \begin{frame}<beamer>
    #+LaTex_HEADER: \frametitle{Outline}
    #+LaTex_HEADER: \tableofcontents[currentsection]
    #+LaTex_HEADER: \end{frame}
    #+LaTex_HEADER: }
    #+LaTex_HEADER: \setcounter{tocdepth}{1}

    #+LaTeX_HEADER: \setbeamertemplate{headline}{}
    #+LaTeX_HEADER: \setbeamertemplate{footline}{
    #+LaTeX_HEADER: \leavevmode%
    #+LaTeX_HEADER: \hbox{%
    #+LaTeX_HEADER: \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{fg=black}%
    #+LaTeX_HEADER:     \usebeamerfont{section in head/foot}\insertsection\hspace*{2em}
    #+LaTeX_HEADER:     \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
    #+LaTeX_HEADER: \end{beamercolorbox}%
    #+LaTeX_HEADER: }%
    #+LaTeX_HEADER: \vskip0pt%
    #+LaTeX_HEADER: }
    #+LaTeX_HEADER: \usepackage{appendixnumberbeamer}

    #+LaTeX_HEADER: \setbeamersize{text margin left=3mm,text margin right=3mm}
*** footnote citations
    #+LaTeX_HEADER: \newcommand\blfootnote[1]{%
    #+LaTeX_HEADER: \begingroup
    #+LaTeX_HEADER: \renewcommand\thefootnote{}\footnote{#1}%
    #+LaTeX_HEADER: \addtocounter{footnote}{-1}%
    #+LaTeX_HEADER:  \endgroup
    #+LaTeX_HEADER: }
    #+LaTeX_HEADER: \setbeamerfont{footnote}{size=\tiny}
*** other imports
    #+LaTeX_HEADER: \usepackage{tikz}
    #+LaTeX_HEADER: \usepackage[retainorgcmds]{IEEEtrantools}
    #+LaTeX_HEADER: \hypersetup{colorlinks=true, allcolors=., urlcolor=blue}
    #+LaTeX_HEADER: \usepackage[absolute,overlay]{textpos}
*** math operators
    #+LaTex_HEADER: \newcommand{\eg}{e.g.\,}
    #+LaTex_HEADER: \newcommand{\ie}{i.e.\,}
    #+LaTex_HEADER: \newcommand{\aka}{a.k.a.\,}
    #+LaTex_HEADER: \newcommand{\etc}{\emph{etc.}\,}

    #+LaTex_HEADER: \newcommand{\X}{{\mathbold X}}
    #+LaTex_HEADER: \newcommand{\bS}{{\mathbold S}}
    #+LaTex_HEADER: \newcommand{\bSigma}{{\mathbold \Sigma}}
    #+LaTex_HEADER: \newcommand{\x}{{\mathbold x}}
    #+LaTex_HEADER: \newcommand{\bbeta}{{\mathbold \beta}}
    #+LaTex_HEADER: \newcommand{\Y}{{\mathbold Y}}
    #+LaTex_HEADER: \newcommand{\y}{{\mathbold y}}
    #+LaTex_HEADER: \newcommand{\B}{{\mathbold B}}
    #+LaTex_HEADER: \newcommand{\W}{{\mathbold W}}
    #+LaTex_HEADER: \newcommand{\U}{{\mathbold U}}
    #+LaTex_HEADER: \newcommand{\V}{{\mathbold V}}
    #+LaTex_HEADER: \newcommand{\bH}{{\mathbold H}}
    #+LaTex_HEADER: \newcommand{\R}{\mathbb{R}}
    #+LaTex_HEADER: \DeclareMathOperator*{\argmin}{argmin}
    #+LaTex_HEADER: \DeclareMathOperator*{\argmax}{argmax}
    #+LaTex_HEADER: \DeclareMathOperator*{\tv}{TV}
    #+LaTex_HEADER: \DeclareMathOperator*{\Tr}{Tr}
    #+LaTex_HEADER: \DeclareMathOperator*{\FFT}{FFT}
    #+LaTex_HEADER: \DeclareMathOperator*{\IFFT}{IFFT}
    #+LaTex_HEADER: \DeclareMathOperator*{\diag}{diag}
    #+LaTex_HEADER: \DeclareMathOperator*{\supp}{supp}
    #+LaTex_HEADER: \DeclareMathOperator*{\tf}{tf}
    #+LaTex_HEADER: \DeclareMathOperator*{\idf}{idf}
    #+LaTex_HEADER: \DeclareMathOperator*{\df}{df}
    #+LaTex_HEADER: \DeclareMathOperator*{\Var}{Var}
    #+LaTex_HEADER: \DeclareMathOperator*{\Frob}{Frob}
    #+LaTex_HEADER: \DeclareMathOperator*{\F}{F}
    #+LaTex_HEADER: \DeclareMathOperator*{\softmax}{softmax}
    #+LaTex_HEADER: \DeclareMathOperator*{\AUC}{AUC}

    #+LaTeX_HEADER: \usepackage{bm}

** color theme
   # #+BEAMER_COLOR_THEME: dove
   # #+BEAMER_COLOR_THEME: seagull

   #+LaTeX_HEADER: \usecolortheme{dove}
   #+LaTeX_HEADER: \setbeamercolor*{block title example}{fg=black,bg=white}
   #+LaTeX_HEADER: \setbeamercolor*{block body example}{fg=black,bg=white}
* Introduction: cross-validation
** Recap of part 1
*** Recap of part 1
**** Supervised learning
       - Regression: least-squares linear regression
       - Classification: logistic regression
#+ATTR_LATEX: :height .4 \textheight :center
[[file:figures/generated/linear_regression_1d/linear_regression.pdf]]
#+ATTR_LATEX: :height .4 \textheight :center
[[file:figures/generated/logistic_regression_1d/logistic_regression.pdf]]

*** Recap of part 1
**** Supervised learning
     :PROPERTIES:
     :BEAMER_act: <1->
     :END:
       - Regression: least-squares linear regression
       - Classification: logistic regression
**** Regularization
     :PROPERTIES:
     :BEAMER_act: <1->
     :END:
       - \(\ell_2\) \aka ridge regularization
**** Model evaluation and selection
     :PROPERTIES:
     :BEAMER_act: <2->
     :END:
       - Out-of-sample generalization; independent test set
       - Performance metrics:
         - regression: mean squared error
         - classification: accuracy, ROC curve
       - Cross-validation
*** Notation & vocabulary
**** Supervised learning framework
 \begin{equation}
 Y = f(X) + E
 \end{equation}
\vspace{-10pt}
#+ATTR_BEAMER: :overlay +-
 - \(Y \in \R \): output (\aka target, dependent variable) to predict
 - \(X \in \R^p \): features (\aka inputs, regressors, descriptors, independent variables)
 - \(E \in \R \): unmodelled noise
 - \(f\): the function we try to approximate
**** Example: linear regression
     :PROPERTIES:
     :BEAMER_act: <4->
     :END:
\vspace{-20pt}
 \begin{IEEEeqnarray}{rCl}
 Y & = & \beta_0 + \langle X, \beta \rangle + E \\
& = & \beta_0 + \sum_{j=1}^p X_j \, \beta_j + E
 \end{IEEEeqnarray}
"learning" = estimating \(\beta_0 \in \R\) and \(\beta \in \R^p\)
*** How to estimate parameters: Empirical Risk Minimization
Given \(n\) training examples \(\X \in \R^{n \times p}\), \(\y \in \R^n\),

find \(\hat{\beta}_0 \in \R, \hat{\bbeta} \in \R^p\)

that minimize the empirical risk:
\begin{IEEEeqnarray}{rcl}
\| \y - \hat{\y} \|_2^2 & \; = \; & \| \y - \hat{\beta}_0 - \X \, \hat{\bbeta} \|_2^2 \\
& \; = \; & \sum_{i=1}^n (\y_i - \hat{\beta}_0 - \sum_{j=1}^p \X_{ij}\, \hat{\bbeta}_j )^2
\end{IEEEeqnarray}

"Fitting" the parameters to \(\X, \y\).
*** Need for fresh test data
    When you hear "best", "maximum", "select", ... think "bias"
- I have 4 dice and want to find one that rolls high numbers
- I roll them all once and select the die that gives the highest number
- The selected die rolled a 5. Is 5 a good estimate of that die's average result? What if I had 1,000 dice?
- I need to roll it again to get an unbiased estimate
#+ATTR_LATEX: :height .5 \textheight
[[file:figures/generated/regression_to_mean/dice_rolls.pdf]]
*** Estimating prediction performance
When you hear "best", "maximum", "select", ... think "bias"
**** Setting the parameters
     - *Select* \(\bbeta\) that gives the *best* prediction on training data
     - The prediction score for \(\hat{\bbeta}\) is biased: compute a new score on unseen test data.
** Supervised learning with sklearn
*** scikit-learn "estimator API": =fit; predict=
  #+BEGIN_EXAMPLE
  estimator = Ridge()
  estimator.fit(X_train, y_train)
  predictions = estimator.predict(X_test)
  #+END_EXAMPLE
  \vfill
  https://scikit-learn.org/stable/getting_started.html
  [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html][=sklearn.linear_model.Ridge=]]

*** Evaluating performance with =sklearn.metrics=
  #+BEGIN_EXAMPLE
  estimator = Ridge()
  estimator.fit(X_train, y_train)
  predictions = estimator.predict(X_test)

  mse = metrics.mean_squared_error(y_test, predictions)
  #+END_EXAMPLE
  \vfill
  https://scikit-learn.org/stable/getting_started.html

  [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html][=sklearn.linear_model.Ridge=]]

  [[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics][=sklearn.metrics=]]

  [[https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules][more info on model evaluation]]
  \vfill
  =ex_01_fit_predict.py=

** cv
*** Cross-validation
  #+ATTR_LATEX: :height .7 \textheight
  [[file:figures/generated/cv_figure_simple.pdf]]

  [[https://scikit-learn.org/stable/modules/cross_validation.html][=scikit-learn.org/stable/modules/cross_validation.html=]]
  [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=sklearn.model_selection.cross_validate=]]
  # https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py
  =ex_02_cross_validate.py=
* Model and hyperparameter selection
** nested cv
*** Regularization

**** Example: ridge regression
\begin{equation}
\argmin_{\bbeta, \beta_0} \| \y - \beta_0 - \X \, \bbeta \| + \alpha \, \|\bbeta\|_2^2
\end{equation}
*** Example hyperparameter: regularization                      :B_fullframe:
    :PROPERTIES:
    :BEAMER_env: fullframe
    :END:
**** var                                                              :BMCOL:
     :PROPERTIES:
     :BEAMER_col: .33
     :END:
  \(\small{ \text{Var}(\hat{\beta}_i) = \mathbb{E}(\hat{\beta}_i  - \mathbb{E}(\hat{\beta}_i))^2} \)

**** plot                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: .38
     :END:
\vspace{-15pt}
     #+ATTR_LATEX: :height \textheight
     [[file:figures/generated/ridge_regularization_path/ridge_regularization_path.pdf]]
**** bias                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: .3
     :END:
  \(\small \text{Bias}(\hat{\beta}_i) = \mathbb{E}(\hat{\beta}_i) - \beta_i\)

*** Setting hyperparameters
**** How can we choose the ridge hyperparameter \(\alpha\)?
**** answer                                                 :B_ignoreheading:
     :PROPERTIES:
     :BEAMER_env: ignoreheading
     :END:
     Try a few and pick the best one...

     But measure its performance on separate data!
*** Nested cross-validation
When you hear "best", "maximum", "select", ... think "bias"
**** Setting the parameters
    :PROPERTIES:
    :BEAMER_act: <2->
    :END:
     - *Select* \(\bbeta\) that gives the *best* prediction on training data
     - The prediction score for \(\hat{\bbeta}\) is biased: compute a new score on unseen test data.
**** Setting the hyperparameters
    :PROPERTIES:
    :BEAMER_act: <3->
    :END:
     - Repeat step 1 for a few values of \(\alpha\), fitting and testing several models
     - *Select* the hyperparameter that obtains the *best* prediction on test data
     - The prediction score of that model on /test/ data is biased: evaluate it again on unseen data
*** One split
[[file:figures/generated/train_eval_test/datasets.pdf]]
*** Nested cross-validation
[[file:figures/generated/cv_figure_nested.pdf]]
  see  [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=sklearn.model_selection.GridSearchCV=]]

*** Implementing nested CV
    =ex_05_nested_cross_validation.py=
* Dimensionality reduction
** Intro
*** Dimensionality reduction
**** Until now
     #+ATTR_LATEX: :height .12 \textheight
     [[file:figures/graphs/pipeline-1.pdf]]
**** Add a step in the pipeline: simplifying the inputs
     #+ATTR_LATEX: :height .12 \textheight
     [[file:figures/graphs/pipeline-2.pdf]]
*** Dimensionality reduction
    \begin{equation}
    \hat{\y} = \hat{\beta}_0 + \hat{\bbeta}_1 \, \X_{:, 1} + \hat{\bbeta}_2 \, \X_{:, 1} + \dots + \hat{\bbeta}_p \, \X_{:, p}
    \end{equation}
**** Problems when the number of features \(p\) becomes large
     - Bigger errors on test data (larger variance of predictions)
     - Numerical stability issues
     - Computational cost and memory usage
*** Simulated data for linear regression
    - Generate \(\X \in \R^{n \times 3}\), \(\mathbold{\bbeta} \in \R^3\), \(\mathbold{e} \in \R^n\) and \(\y = \X \, \bbeta + \mathbold{e} \in R^n\)
    - Append columns containing random noise to \(\X\)
    - Now \(\X \in \R^{n \times p}\), with \(p \geq 3\), but only the first 3 columns are linked with \(\y\)
    - Split into training and testing tests and evaluate a linear regression model: what happens when \(p\) becomes large?
  # \vfill

See [[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression][=sklearn.datasets.make_regression=]] for generating data
#+ATTR_LATEX: :height .4 \textheight
[[file:figures/generated/show_make_regression/x_construction.pdf]]
*** Model complexity: overfitting
    - Model complexity increases with dimension.
    - Example: a linear model in dimension \(p\) can fit exactly (0 training error) any set of \(p + 1\) points.
    - Risk of overfitting: fitting exactly training data but failing on test data

    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse_log.pdf]]
** Univariate feature selection
*** Univariate feature selection
    - \aka feature screening, filtering ...
    - Check features (columns of \(\X\)) one by one for association with the output \(\y\)
    - Keep only a fixed number or percentage of the features
**** Simple (linear) association criteria
     - for regression: correlation
     - for classification: ANalysis Of VAriance
**** Read more in the scikit-learn user guide
     [[https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection][scikit-learn feature selection]]

*** Univariate feature selection
    Keeping only the 10 best features (most correlated with \(\y\))
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/ridge_overfitting/mse_with_dim_reduction_log.pdf]]

** Fit whole pipeline on train data only
*** Dataset transformations
**** Typical pipeline
[[file:figures/graphs/pipeline-2-no-color.pdf]]
**** Example
[[file:figures/graphs/pipeline-3.pdf]]
*** scikit-learn "transformer API": =fit; transform=
    #+begin_example
  transformer = SelectKBest()
  transformer.fit(X_train)
  transformed_X = transformer.transform(X_train)
    #+end_example
**** can also be written:
     #+begin_example
  transformer = SelectKBest()
  transformed_X = transformer.fit_transform(X_train)
     #+end_example
**** links                                                   :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
   \vfill

   [[https://scikit-learn.org/stable/modules/feature_selection.html][scikit-learn feature selection]]

  \vfill

*** =feature_selection.SelectKBest=
**** =fit:=
     - compute ANOVA or correlation for each column of \(X\)
     - Remember the indices of the \(k\) columns with highest scores
**** =transform:=
     - Index input to keep only the \(k\) selected columns


**** link                                                    :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
  [[https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest][=sklearn.feature_selection.SelectKBest=]]



*** scikit-learn "transformer API": =fit; transform=
    #+begin_example
  transformer = SelectKBest()
  transformed_X_train = transformer.fit_transform(X_train)

  transformed_X_test = transformer.transform(X_test)
    #+end_example

\vfill
  =ex_04_feature_selection.py=
** Linear decomposition methods
*** Solution 2: linear decomposition methods
**** Maybe OK to drop $\X_2$:
     \vspace{-10pt}
     #+ATTR_LATEX: :height .3\textheight
     [[file:figures/generated/pca/cloud_aligned.pdf]]
     \vspace{-20pt}
**** Data low-dimensional but no feature can be dropped:
     #+ATTR_LATEX: :height .3\textheight
     [[file:figures/generated/pca/cloud_not_aligned.pdf]]

Find a better referential in which to represent the data
*** COMMENT Linear regression: projection on the column space of \(\X\)
**** Approximate \(y\) as a combination of the columns of \(X\)
  \begin{equation}
  \hat{\y} = \X \, \hat{\bbeta} \in \R^n
  \end{equation}
- The columns of \(X\) are a family of \(p\) \(n\)-dimensional vectors
- When \(p\) is high or the columns of \(X\) are correlated, we want to use a family of \(k < p\) instead
- Feature selection: drop some columns, keep only \(k\)
- Could we build a better family of \(k\) vectors?
*** Linear regression: projection on the column space of \(X\)
**** top                                                     :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .3
      :END:
      \begin{equation}
      \hat{\y} = \X \, \hat{\bbeta}
      \end{equation}

***** equation                                               :B_column:BMCOL:
      :PROPERTIES:
      :BEAMER_env: column
      :BEAMER_col: .7
      :END:
      \vspace{-17pt}
      #+ATTR_LATEX: :height .7\textheight
      [[file:figures/generated/dim_reduction_colors/regression_full_3.pdf]]

**** bottom                                                  :B_structureenv:
     :PROPERTIES:
     :BEAMER_env: structureenv
     :END:
     - Too many features: high variance & unstable solution
     - Feature selection: drop some columns of \(\X\)
     - Other ways to build a family of \(k\) vectors on which to regress \(\y\)?
*** Linear decomposition: low-rank approximation of \(\X\)
    Minimize
\begin{equation}
\| \X - \W \, \bH \|_{\F}^2 = \sum_{i, j} ( \X_{i,j} - (\W \, \bH)_{i,j})^2
\end{equation}
    #+ATTR_LATEX: :height .5\textheight
    [[file:figures/generated/dim_reduction_colors/factorization_3.pdf]]
*** Linear regression after dimensionality reduction
    \begin{equation}
    \hat{\y} = \W \, \hat{\bbeta}
    \end{equation}
    #+ATTR_LATEX: :height .7\textheight
    [[file:figures/generated/dim_reduction_colors/regression_reduced_3.pdf]]
*** Prediction for a new data point \(\x \in \R^{p}\)
    - Find the combination of rows of \(\bH\) that is closest to \(\x\): regress \(\x\) on \(\bH^T\)
    - Multiply by \(\hat{\bbeta}\)
    \begin{equation}
\x \in \R^p \rightarrow \text{projection} \rightarrow \mathbold{w} \in \R^k \rightarrow \langle \cdot \, , \, \hat{\bbeta}\rangle \rightarrow \hat{y} \in \R
    \end{equation}
*** Principal Component Analysis
    - Singular Value Decomposition of \(\X\):
    \begin{equation}
    \X = \U \, \bS \, \V^T
    \end{equation}
    with \(\X \in \R^{n \times p}\), \(\U \in \R^{n \times r}\), \(\bS \in \R^{r \times r}\), \(\V \in \R^{r \times p}\)
    - \(r = \min(n, p)\)
    - \(\bS \succeq 0\) diagonal with decreasing values \(s_j\) along the diagonal
    - \(\U^T\, \U = I_r\)
    - \(\V^T\, \V = I_r\)

Truncating the SVD to keep only the first \(k\) components gives the best rank-\(k\) approximation of \(\X\)
#+ATTR_LATEX: :height .3\textheight
[[file:figures/generated/pca/cloud_not_aligned_with_pc.pdf]]
*** Singular Value Decomposition
\begin{equation}
\X = \U \, \bS \, \V^T
\end{equation}
#+ATTR_LATEX: :height .5 \textheight :center
[[file:figures/generated/pca_step_by_step/pca_steps_1.pdf]]

\begin{equation}
\U^T \, \U = I_p
\end{equation}
\begin{equation}
\V^T \, \V = I_p
\end{equation}

*** Singular Value Decomposition
\begin{equation}
\X = \U \, \bS \, \V^T
\end{equation}
#+ATTR_LATEX: :height .5 \textheight :center
[[file:figures/generated/pca_step_by_step/pca_steps_2.pdf]]

\begin{equation}
\U^T \, \U = I_p
\end{equation}
\begin{equation}
\V^T \, \V = I_p
\end{equation}


*** Singular Value Decomposition
\begin{equation}
\X = \U \, \bS \, \V^T
\end{equation}
#+ATTR_LATEX: :height .5 \textheight :center
[[file:figures/generated/pca_step_by_step/pca_steps_3.pdf]]

\begin{equation}
\U^T \, \U = I_p
\end{equation}
\begin{equation}
\V^T \, \V = I_p
\end{equation}

*** Other decomposition methods
Many other methods use the same objective (sum of squared reconstruction errors), but add penalties or constraints on the factors
- Dictionary Learning
- Non-negative Matrix Factorization
- K-means clustering
- ...

**** What about \(\y\)?
     - PCA is an example of /unsupervised/ learning: it does not use \(\y\)
     - Some other methods take it into account: \eg Partial Least Squares
*** Ridge regression and PCA
    - Both ridge regression and PC regression compute the coordinates of \(\y\) in the basis given by the SVD of \(\X\)
    - Ridge shrinks the coordinate along \(\U_j\) by a factor \(s_j^2 / (s_j^2 + \alpha)\)
    - PC regression sets the coordinates to 0 except for those corresponding to the \(k\) largest \(s_j\): shrinks by a factor \(\mathbold{1}_{\{j \leq k\}}\)

#+ATTR_LATEX: :height .6\textheight
[[file:figures/generated/dim_reduction_colors/regression_reduced_3_svd.pdf]]
* Conclusion
*** Some pitfalls with cross-validation
**** Overfitting the hyperparameters
       + select hyperparameters with nested CV [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html][=sklearn.model_selection.GridSearchCV=]]
**** Fitting part of the pipeline on the whole dataset
       + use  [[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html][=sklearn.pipeline.Pipeline=]]
**** Ignoring dependencies between samples
         + e.g. time series: use appropriate [[https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators][cross-validation iterator]]
**** Ignoring dependencies between CV scores
         + Training sets overlap: cross-validation scores of different splits are not independent
**** Over-interpreting good CV scores
         + Good CV scores do not mean the model will always perform well on a new dataset

*** Split choice example: time series
Which is easier?
#+ATTR_LATEX: :height .4 \textheight
[[file:figures/generated/time_series_cv/kfold.pdf]]

#+ATTR_LATEX: :height .4 \textheight
[[file:figures/generated/time_series_cv/kfold_shuffled.pdf]]
*** Remember that CV training sets overlap
    #+ATTR_LATEX: :height .6 \textheight
[[file:figures/generated/train_eval_test/cv_not_nested.pdf]]

So the scores are not independent! Their variance can be underestimated.
